{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552203a1-8a77-4729-9c4e-405fe5c168a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import librosa.display\n",
    "\n",
    "import plotly.express as px\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from librosa import feature, amplitude_to_db, load\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Activation , Dropout\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9daae541-0355-4972-9cbf-72e32b57b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/119 files (0.84%)\n",
      "Processed 2/119 files (1.68%)\n",
      "Processed 3/119 files (2.52%)\n",
      "Processed 4/119 files (3.36%)\n",
      "Processed 5/119 files (4.20%)\n",
      "Processed 6/119 files (5.04%)\n",
      "Processed 7/119 files (5.88%)\n",
      "Processed 8/119 files (6.72%)\n",
      "Processed 9/119 files (7.56%)\n",
      "Processed 10/119 files (8.40%)\n",
      "Processed 11/119 files (9.24%)\n",
      "Processed 12/119 files (10.08%)\n",
      "Processed 13/119 files (10.92%)\n",
      "Processed 14/119 files (11.76%)\n",
      "Processed 15/119 files (12.61%)\n",
      "Processed 16/119 files (13.45%)\n",
      "Processed 17/119 files (14.29%)\n",
      "Processed 18/119 files (15.13%)\n",
      "Processed 19/119 files (15.97%)\n",
      "Processed 20/119 files (16.81%)\n",
      "Processed 21/119 files (17.65%)\n",
      "Processed 22/119 files (18.49%)\n",
      "Processed 23/119 files (19.33%)\n",
      "Processed 24/119 files (20.17%)\n",
      "Processed 25/119 files (21.01%)\n",
      "Processed 26/119 files (21.85%)\n",
      "Processed 27/119 files (22.69%)\n",
      "Processed 28/119 files (23.53%)\n",
      "Processed 29/119 files (24.37%)\n",
      "Processed 30/119 files (25.21%)\n",
      "Processed 31/119 files (26.05%)\n",
      "Processed 32/119 files (26.89%)\n",
      "Processed 33/119 files (27.73%)\n",
      "Processed 34/119 files (28.57%)\n",
      "Processed 35/119 files (29.41%)\n",
      "Processed 36/119 files (30.25%)\n",
      "Processed 37/119 files (31.09%)\n",
      "Processed 38/119 files (31.93%)\n",
      "Processed 39/119 files (32.77%)\n",
      "Processed 40/119 files (33.61%)\n",
      "Processed 41/119 files (34.45%)\n",
      "Processed 42/119 files (35.29%)\n",
      "Processed 43/119 files (36.13%)\n",
      "Processed 44/119 files (36.97%)\n",
      "Processed 45/119 files (37.82%)\n",
      "Processed 46/119 files (38.66%)\n",
      "Processed 47/119 files (39.50%)\n",
      "Processed 48/119 files (40.34%)\n",
      "Processed 49/119 files (41.18%)\n",
      "Processed 50/119 files (42.02%)\n",
      "Processed 51/119 files (42.86%)\n",
      "Processed 52/119 files (43.70%)\n",
      "Processed 53/119 files (44.54%)\n",
      "Processed 54/119 files (45.38%)\n",
      "Processed 55/119 files (46.22%)\n",
      "Processed 56/119 files (47.06%)\n",
      "Processed 57/119 files (47.90%)\n",
      "Processed 58/119 files (48.74%)\n",
      "Processed 59/119 files (49.58%)\n",
      "Processed 60/119 files (50.42%)\n",
      "Processed 61/119 files (51.26%)\n",
      "Processed 62/119 files (52.10%)\n",
      "Processed 63/119 files (52.94%)\n",
      "Processed 64/119 files (53.78%)\n",
      "Processed 65/119 files (54.62%)\n",
      "Processed 66/119 files (55.46%)\n",
      "Processed 67/119 files (56.30%)\n",
      "Processed 68/119 files (57.14%)\n",
      "Processed 69/119 files (57.98%)\n",
      "Processed 70/119 files (58.82%)\n",
      "Processed 71/119 files (59.66%)\n",
      "Processed 72/119 files (60.50%)\n",
      "Processed 73/119 files (61.34%)\n",
      "Processed 74/119 files (62.18%)\n",
      "Processed 75/119 files (63.03%)\n",
      "Processed 76/119 files (63.87%)\n",
      "Processed 77/119 files (64.71%)\n",
      "Processed 78/119 files (65.55%)\n",
      "Processed 79/119 files (66.39%)\n",
      "Processed 80/119 files (67.23%)\n",
      "Processed 81/119 files (68.07%)\n",
      "Processed 82/119 files (68.91%)\n",
      "Processed 83/119 files (69.75%)\n",
      "Processed 84/119 files (70.59%)\n",
      "Processed 85/119 files (71.43%)\n",
      "Processed 86/119 files (72.27%)\n",
      "Processed 87/119 files (73.11%)\n",
      "Processed 88/119 files (73.95%)\n",
      "Processed 89/119 files (74.79%)\n",
      "Processed 90/119 files (75.63%)\n",
      "Processed 91/119 files (76.47%)\n",
      "Processed 92/119 files (77.31%)\n",
      "Processed 93/119 files (78.15%)\n",
      "Processed 94/119 files (78.99%)\n",
      "Processed 95/119 files (79.83%)\n",
      "Processed 96/119 files (80.67%)\n",
      "Processed 97/119 files (81.51%)\n",
      "Processed 98/119 files (82.35%)\n",
      "Processed 99/119 files (83.19%)\n",
      "Processed 100/119 files (84.03%)\n",
      "Processed 101/119 files (84.87%)\n",
      "Processed 102/119 files (85.71%)\n",
      "Processed 103/119 files (86.55%)\n",
      "Processed 104/119 files (87.39%)\n",
      "Processed 105/119 files (88.24%)\n",
      "Processed 106/119 files (89.08%)\n",
      "Processed 107/119 files (89.92%)\n",
      "Processed 108/119 files (90.76%)\n",
      "Processed 109/119 files (91.60%)\n",
      "Processed 110/119 files (92.44%)\n",
      "Processed 111/119 files (93.28%)\n",
      "Processed 112/119 files (94.12%)\n",
      "Processed 113/119 files (94.96%)\n",
      "Processed 114/119 files (95.80%)\n",
      "Processed 115/119 files (96.64%)\n",
      "Processed 116/119 files (97.48%)\n",
      "Processed 117/119 files (98.32%)\n",
      "Processed 118/119 files (99.16%)\n",
      "Processed 119/119 files (100.00%)\n",
      "\n",
      "✅ All files processed. Mean MFCCs saved to mfcc_means_Dementia_new.csv\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to extract mean MFCC features\n",
    "def extract_mean_mfcc(audio_file, n_mfcc=20, frame_length_ms=25, hop_length_ms=10):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from an audio file and computes the mean across frames.\n",
    "    Returns the mean MFCC features as a list.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None)  \n",
    "\n",
    "    # Compute hop length and FFT window size\n",
    "    hop_length = int((hop_length_ms / 1000) * sr)  \n",
    "    n_fft = int((frame_length_ms / 1000) * sr)    \n",
    "\n",
    "    # Extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Compute the mean of MFCCs across time frames\n",
    "    mean_mfccs = np.mean(mfccs, axis=1)\n",
    "\n",
    "    return mean_mfccs\n",
    "\n",
    "# Function to process all audio files in a folder for multiple frame lengths\n",
    "def process_audio_folder(input_folder, hop_lengths=[25, 50, 100, 150, 200, 250, 300], output_csv=\"mfcc_means.csv\"):\n",
    "    \"\"\"\n",
    "    Processes all audio files in the input folder for different frame lengths,\n",
    "    extracts mean MFCCs, and saves results to a CSV.\n",
    "    \"\"\"\n",
    "    # Get all audio files in the folder\n",
    "    audio_files = [f for f in os.listdir(input_folder) if f.endswith(('.wav', '.mp3'))]\n",
    "    total_files = len(audio_files)\n",
    "    \n",
    "    if total_files == 0:\n",
    "        print(\"No audio files found in the folder!\")\n",
    "        return\n",
    "    \n",
    "    # List to store results\n",
    "    mfcc_results = []\n",
    "\n",
    "    # Process each audio file\n",
    "    for i, audio_file in enumerate(audio_files):\n",
    "        audio_path = os.path.join(input_folder, audio_file)\n",
    "        \n",
    "        for frame_length in frame_lengths:\n",
    "            mean_mfccs = extract_mean_mfcc(audio_path, frame_length_ms=frame_length)\n",
    "            \n",
    "            # Append results with filename and frame length\n",
    "            mfcc_results.append([audio_file, frame_length] + mean_mfccs.tolist())\n",
    "\n",
    "        # Display progress\n",
    "        print(f\"Processed {i + 1}/{total_files} files ({(i + 1) / total_files * 100:.2f}%)\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    column_names = [\"Filename\", \"Frame_Length_ms\"] + [f\"MFCC_{i+1}\" for i in range(len(mean_mfccs))]\n",
    "    mfcc_df = pd.DataFrame(mfcc_results, columns=column_names)\n",
    "\n",
    "    # Save to CSV\n",
    "    mfcc_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n✅ All files processed. Mean MFCCs saved to {output_csv}\")\n",
    "\n",
    "# Define input folder containing audio files\n",
    "input_folder = r\"C:\\Users\\adity\\OneDrive\\Desktop\\Speech Sample\\Dementia\"  # Replace with your actual folder path\n",
    "output_file = \"mfcc_means_Dementia_new.csv\"  # Output file\n",
    "\n",
    "# Process all files for multiple frame lengths and save mean MFCCs\n",
    "process_audio_folder(input_folder, output_csv=output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01922cea-a948-4bba-b9e5-99fd84c88fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Filename', 'Frame_Length_ms', 'MFCC_1', 'MFCC_2', 'MFCC_3', 'MFCC_4', 'MFCC_5', 'MFCC_6', 'MFCC_7', 'MFCC_8', 'MFCC_9', 'MFCC_10', 'MFCC_11', 'MFCC_12', 'MFCC_13', 'MFCC_14', 'MFCC_15', 'MFCC_16', 'MFCC_17', 'MFCC_18', 'MFCC_19', 'MFCC_20']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\adity\\mfcc_means_control.csv')\n",
    "\n",
    "# Get column names\n",
    "columns = df.columns.tolist()\n",
    "print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab647dc8-8fbd-47dc-b85d-1a38570734c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved as 'merged_mfcc_new.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read both CSV files\n",
    "df1 = pd.read_csv('mfcc_means_control_new.csv')\n",
    "df2 = pd.read_csv('mfcc_means_Dementia_new.csv')\n",
    "\n",
    "# Add an output column: 0 for the first file, 1 for the second\n",
    "df1['output'] = 0\n",
    "df2['output'] = 1\n",
    "\n",
    "# Merge the data\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the merged CSV\n",
    "merged_df.to_csv('merged_mfcc_new.csv', index=False)\n",
    "\n",
    "print(\"Merged CSV file saved as 'merged_mfcc_new.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521d0a96-948d-4654-b8e3-aa0933c5f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_10.csv\n",
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_25.csv\n",
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_50.csv\n",
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_100.csv\n",
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_200.csv\n",
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_300.csv\n",
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_400.csv\n",
      "Saved: C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length/Hop_Length_500.csv\n",
      "All files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File path\n",
    "file_path = \"merged_mfcc_new_hop_length.csv\"\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the output folder exists\n",
    "output_folder = r\"C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Values to filter\n",
    "hop_lengths = [10, 25, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "# Split and save separate CSV files\n",
    "for value in hop_lengths:\n",
    "    subset_df = df[df['Hop_Length_ms'] == value]  # Filter rows\n",
    "    if not subset_df.empty:\n",
    "        filename = f\"{output_folder}/Hop_Length_{value}.csv\"\n",
    "        subset_df.to_csv(filename, index=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"All files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e42886f-e607-4ab7-a641-22d2cedfe60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: Hop_Length_10.csv\n",
      "Updated: Hop_Length_100.csv\n",
      "Updated: Hop_Length_200.csv\n",
      "Updated: Hop_Length_25.csv\n",
      "Updated: Hop_Length_300.csv\n",
      "Updated: Hop_Length_400.csv\n",
      "Updated: Hop_Length_50.csv\n",
      "Updated: Hop_Length_500.csv\n",
      "All files updated successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = r\"C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length\"\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = ['Filename', 'Hop_Length_ms']  # Change these to actual column names\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):  # Ensure it's a CSV file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop specified columns if they exist\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "        \n",
    "        # Save the modified CSV\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Updated: {filename}\")\n",
    "\n",
    "print(\"All files updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29030394-0e46-46bc-9433-9a428a35dfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on Hop_Length_10.csv - Accuracy: 0.6078\n",
      "Model trained on Hop_Length_100.csv - Accuracy: 0.6078\n",
      "Model trained on Hop_Length_200.csv - Accuracy: 0.5882\n",
      "Model trained on Hop_Length_25.csv - Accuracy: 0.6078\n",
      "Model trained on Hop_Length_300.csv - Accuracy: 0.6078\n",
      "Model trained on Hop_Length_400.csv - Accuracy: 0.5882\n",
      "Model trained on Hop_Length_50.csv - Accuracy: 0.6078\n",
      "Model trained on Hop_Length_500.csv - Accuracy: 0.6078\n",
      "SVM applied to all CSV files successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = r\"C:\\Users\\adity\\OneDrive\\Desktop\\Hop2length\"\n",
    "\n",
    "# Iterate through all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure the target column 'output' exists\n",
    "        if 'output' not in df.columns:\n",
    "            print(f\"Skipping {filename} (No 'output' column)\")\n",
    "            continue\n",
    "        \n",
    "        # Separate features (X) and target variable (y)\n",
    "        X = df.drop(columns=['output'])  # Drop target column\n",
    "        y = df['output']\n",
    "\n",
    "        # Handle missing values (fill with mean)\n",
    "        X = X.fillna(X.mean())\n",
    "\n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train SVM model\n",
    "        model = SVC(kernel='rbf')  # Use RBF kernel for non-linear classification\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Model trained on {filename} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"SVM applied to all CSV files successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511ea38-45a8-4b20-ad06-bbed794dddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
