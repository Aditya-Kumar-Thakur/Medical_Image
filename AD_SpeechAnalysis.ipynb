{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eeb012a-06cf-4cb9-9460-3c02bcafd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import librosa.display\n",
    "\n",
    "import plotly.express as px\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from librosa import feature, amplitude_to_db, load\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Activation , Dropout\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a662d635-65f9-403a-99f7-fc3aab24130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\adity\\OneDrive\\Desktop\\Speech Sample\\Dementia\\001-0.wav\n",
      "Features saved to R1audio_features.csv\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 4.1: Extract MFCC\n",
    "def extract_mfcc(audio_file, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from an audio file.\n",
    "    MFCC(x) â† DCT(log(FFT(x)))\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # MFCC extraction\n",
    "    return mfccs.T  # Transpose to get frame-wise MFCCs\n",
    "\n",
    "# Step 4.2: Extract Prosody Features (e.g., pitch, energy)\n",
    "def extract_prosody_features(audio_file):\n",
    "    \"\"\"\n",
    "    Extract prosody features like pitch and energy from an audio file.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "    # Pitch (Fundamental Frequency)\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch = pitches[pitches > 0].mean()  # Mean pitch\n",
    "    \n",
    "    # Energy (Root Mean Square Energy)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    mean_energy = np.mean(rms)\n",
    "    \n",
    "    return {\"pitch\": pitch, \"energy\": mean_energy}\n",
    "\n",
    "# Step 4.3: Extract Statistical Features\n",
    "def extract_statistical_features(features):\n",
    "    \"\"\"\n",
    "    Extract statistical features (mean, standard deviation, min, max) for given features.\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        \"mean\": np.mean(features, axis=0),\n",
    "        \"std_dev\": np.std(features, axis=0),\n",
    "        \"min\": np.min(features, axis=0),\n",
    "        \"max\": np.max(features, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Wrapper Function to Process Data\n",
    "def process_audio_files(audio_files):\n",
    "    \"\"\"\n",
    "    Process multiple audio files to extract MFCC, prosody, and statistical features.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for file in audio_files:\n",
    "        print(f\"Processing: {file}\")\n",
    "        \n",
    "        # Extract MFCC Features\n",
    "        mfcc = extract_mfcc(file)\n",
    "        mfcc_stats = extract_statistical_features(mfcc)\n",
    "        \n",
    "        # Extract Prosody Features\n",
    "        prosody = extract_prosody_features(file)\n",
    "        \n",
    "        # Combine Features\n",
    "        combined_features = {\n",
    "            **{f\"MFCC_mean_{i+1}\": mfcc_stats[\"mean\"][i] for i in range(len(mfcc_stats[\"mean\"]))},\n",
    "            **{f\"MFCC_std_{i+1}\": mfcc_stats[\"std_dev\"][i] for i in range(len(mfcc_stats[\"std_dev\"]))},\n",
    "            **prosody\n",
    "        }\n",
    "        \n",
    "        all_features.append(combined_features)\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "# Example Usage\n",
    "audio_files = [\n",
    "    r\"C:\\Users\\adity\\OneDrive\\Desktop\\Speech Sample\\Dementia\\001-0.wav\",  # Add paths to your audio files\n",
    "    \n",
    "]\n",
    "\n",
    "features_df = process_audio_files(audio_files)\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = \"R1audio_features.csv\"\n",
    "features_df.to_csv(output_csv, index=False)\n",
    "print(f\"Features saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ae9f0d-ed74-4a88-94a2-93e1d657020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio features saved to Raudio_features.csv\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the audio file\n",
    "audio_file = r\"C:\\Users\\adity\\OneDrive\\Desktop\\Speech Sample\\Dementia\\001-0.wav\"  # Replace with your audio file path\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Step 2: Extract Features (e.g., MFCCs)\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Extract 13 MFCCs\n",
    "\n",
    "# Convert the MFCCs to a DataFrame\n",
    "mfcc_df = pd.DataFrame(mfccs.T, columns=[f\"MFCC_{i+1}\" for i in range(mfccs.shape[0])])\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "# Optional: Add timestamps (each column corresponds to a frame)\n",
    "time_stamps = librosa.frames_to_time(range(mfccs.shape[1]), sr=sr)\n",
    "mfcc_df[\"Timestamp\"] = time_stamps\n",
    "\n",
    "output_csv = \"Raudio_features.csv\"  # Desired CSV file name\n",
    "mfcc_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Audio features saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d91092-9698-4703-b8ed-fd40c3ebc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# data_path = 'audios/UrbanSound8K.csv'\n",
    "data_path = r\"C:\\Users\\adity\\.cache\\kagglehub\\datasets\\chrisfilo\\urbansound8k\\versions\\1\\UrbanSound8K.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b329ac5b-54bc-4432-99a0-76cd78df1650",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# path = 'audios'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124madity\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpeech Sample\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index_num,row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[0;32m      5\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(path),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslice_file_name\u001b[39m\u001b[38;5;124m'\u001b[39m]))    \n\u001b[0;32m      6\u001b[0m     sample_path\u001b[38;5;241m.\u001b[39mappend(file_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "sample_path = []\n",
    "# path = 'audios'\n",
    "path = r\"C:\\Users\\adity\\OneDrive\\Desktop\\Speech Sample\"\n",
    "for index_num,row in tqdm(df.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(path),'fold'+str(row[\"fold\"])+'/',str(row['slice_file_name']))    \n",
    "    sample_path.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00adf7-f011-4836-8217-b2893a09a01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
